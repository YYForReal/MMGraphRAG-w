{
  "image_1": {
    "image_id": 1,
    "image_path": "./example_output/images/image_1.jpg",
    "caption": [],
    "footnote": [],
    "context": "Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task (Sang and De Meulder, 2003; Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class $O$ . Specifically, the number of tokens with tagging class $O$ is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is 1 Introduction Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks. deemphasize easy-negative examples. Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training. With the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP. ",
    "chunk_order_index": 0,
    "chunk_id": "chunk-08d62444b992fc41f1eb900a71e0b520",
    "description": "The image is a table titled 'Table 1: Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.' It provides data on the imbalance between negative and positive examples for various NLP tasks, along with their respective ratios. The table has four columns: 'Task,' '# neg' (number of negative examples), '# pos' (number of positive examples), and 'ratio' (the ratio of negative to positive examples). Here is a detailed breakdown of the rows:\n\n1. **CoNLL03 NER**:\n   - **# neg**: 170K\n   - **# pos**: 34K\n   - **ratio**: 4.98\n\n2. **OntoNotes5.0 NER**:\n   - **# neg**: 1.96M\n   - **# pos**: 239K\n   - **ratio**: 8.18\n\n3. **SQuAD 1.1 (Rajpurkar et al., 2016)**:\n   - **# neg**: 10.3M\n   - **# pos**: 175K\n   - **ratio**: 55.9\n\n4. **SQuAD 2.0 (Rajpurkar et al., 2018)**:\n   - **# neg**: 15.4M\n   - **# pos**: 188K\n   - **ratio**: 82.0\n\n5. **QUOREF (Dasigi et al., 2019)**:\n   - **# neg**: 6.52M\n   - **# pos**: 38.6K\n   - **ratio**: 169\n\nThe table highlights the significant data imbalance across these tasks, with the ratio of negative to positive examples increasing from 4.98 for CoNLL03 NER to 169 for QUOREF. This imbalance is particularly pronounced in machine reading comprehension tasks like SQuAD 1.1, SQuAD 2.0, and QUOREF, where the number of negative examples far exceeds the positive examples. The context provided emphasizes the challenge of data imbalance in NLP tasks and the need for strategies to address this issue, such as the proposed training objective mentioned in the text.",
    "segmentation": false
  },
  "image_2": {
    "image_id": 2,
    "image_path": "./example_output/images/image_2.jpg",
    "caption": [
      "Figure 1: An illustration of derivatives of the four losses. The derivative of DSC approaches zero right after $p$ exceeds 0.5, and for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible. "
    ],
    "footnote": [],
    "context": "To address this issue, we propose to multiply the soft probability Comparing Eq.5 with Eq.11, we can see that Eq.5 is actually a soft form of $F1$ , using a continuous $p$ rather than the binary $\\mathbb{I}(p_{i1}>0.5)$ . This gap isn’t a big issue for balanced datasets, but is extremely detrimental if a big proportion of training examples are easy-negative ones: easy-negative examples can easily dominate training since their probabilities can be pushed to 0 fairly easily. Meanwhile, the model can hardly distinguish between hardnegative examples and positive ones, which has a huge negative effect on the final F1 performance. cient to a more general case. Given two sets $A$ and $B$ , tversky index is computed as follows: $$ \\mathrm{TI}=\\frac{|A\\cap B|}{|A\\cap B|+\\alpha|A\\backslash B|+\\beta|B\\backslash A|} $$ Tversky index offers the flexibility in controlling the tradeoff between false-negatives and falsepositives. It degenerates to DSC if $\\alpha=\\beta=0.5$ The Tversky loss (TL) is thus given as follows: $$ \\mathrm{TL}=\\frac{1}{N}\\sum_{i}\\left[1-\\frac{p_{i1}y_{i1}+\\gamma}{p_{i1}y_{i1}+\\alpha\\,p_{i1}y_{i0}+\\beta\\,p_{i0}y_{i1}+\\gamma}\\right] $$ 3.4 Self-adjusting Dice Loss Consider a simple case where the dataset consists of only one example $x_{i}$ , which is classified as positive as long as $p_{i1}$ is larger than 0.5. The computation of $F1$ score is actually as follows: $$ \\mathrm{F}1(x_{i})=2\\;{\\frac{\\mathbb{I}(p_{i1}>0.5)y_{i1}}{\\mathbb{I}(p_{i1}>0.5)+y_{i1}}} $$ ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-7796c957ecdc053bf1b6da735b8ebe5f",
    "description": "The image is a line graph titled 'Figure 1: An illustration of derivatives of the four losses.' It plots the derivatives of four different loss functions against the probability of the ground-truth label, denoted as \\( \\bar{p}_i \\). The x-axis represents the probability of the ground-truth label, ranging from 0 to 1, while the y-axis represents the derivatives, ranging from -2 to 2. The graph includes four loss functions, each represented by a different color and marker style:\n\n1. **FL (γ=1)**: Represented by a blue line with a triangle marker. The derivative starts at approximately -2 when \\( \\bar{p}_i = 0 \\) and increases sharply as \\( \\bar{p}_i \\) approaches 1, reaching 0 when \\( \\bar{p}_i = 1 \\).\n2. **DL (γ=1)**: Represented by an orange line with a triangle marker. The derivative starts at approximately -1 when \\( \\bar{p}_i = 0 \\) and increases gradually as \\( \\bar{p}_i \\) approaches 1, reaching 0 when \\( \\bar{p}_i = 1 \\).\n3. **TL (β=0.5)**: Represented by a yellow line with a triangle marker. The derivative starts at approximately -1.5 when \\( \\bar{p}_i = 0 \\) and increases gradually as \\( \\bar{p}_i \\) approaches 1, reaching 0 when \\( \\bar{p}_i = 1 \\).\n4. **DSC**: Represented by a purple line with a triangle marker. The derivative starts at approximately -1 when \\( \\bar{p}_i = 0 \\) and increases gradually as \\( \\bar{p}_i \\) approaches 1. Notably, the derivative of DSC approaches zero right after \\( \\bar{p}_i \\) exceeds 0.5, unlike the other losses, which only reach 0 when \\( \\bar{p}_i = 1 \\).\n\nKey observations:\n- For FL, DL, and TL, the derivatives reach 0 only when the probability is exactly 1, indicating that these losses will push \\( \\bar{p}_i \\) to 1 as much as possible.\n- The DSC derivative behaves differently, approaching zero as soon as \\( \\bar{p}_i \\) exceeds 0.5, suggesting that DSC is less sensitive to probabilities above 0.5 compared to the other losses.\n- The graph highlights the differences in how these loss functions respond to changes in the probability of the ground-truth label, with DSC showing a distinct behavior compared to FL, DL, and TL.",
    "segmentation": false
  },
  "image_3": {
    "image_id": 3,
    "image_path": "./example_output/images/image_3.jpg",
    "caption": [],
    "footnote": [
      "Table 3: Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4. "
    ],
    "context": "We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development 4 Experiments In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible.  The intuition of changing $p_{i1}$ to $(1-p_{i1})p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1})p_{i1}$ makes the model attach significantly less focus to them. A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $(1-p)^{\\gamma}$ factor, leading the final loss to be $-(1-p)^{\\gamma}\\log p$ . ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-7796c957ecdc053bf1b6da735b8ebe5f",
    "description": "The image is a table titled 'Experimental results for Chinese POS datasets including CTB5, CTB6, and UD1.4.' It compares the performance of various models on part-of-speech (POS) tagging tasks across three datasets: CTB5, CTB6, and UD1.4. The table includes metrics such as Precision (Prec.), Recall (Rec.), and F1 score for each model. The models listed are:\n\n1. **Joint-POS(Sig)(Shao et al., 2017)**:\n   - CTB5: Prec. = 93.68, Rec. = 94.47, F1 = 94.07\n   - CTB6: Prec. = -, Rec. = -, F1 = 90.81\n   - UD1.4: Prec. = 89.28, Rec. = 89.54, F1 = 89.41\n\n2. **Joint-POS(Ens)(Shao et al., 2017)**:\n   - CTB5: Prec. = 93.95, Rec. = 94.81, F1 = 94.38\n   - CTB6: Prec. = -, Rec. = -, F1 = -\n   - UD1.4: Prec. = 89.67, Rec. = 89.86, F1 = 89.75\n\n3. **Lattice-LSTM(Zhang and Yang, 2018)**:\n   - CTB5: Prec. = 94.77, Rec. = 95.51, F1 = 95.14\n   - CTB6: Prec. = 92.00, Rec. = 90.86, F1 = 91.43\n   - UD1.4: Prec. = 90.47, Rec. = 89.70, F1 = 90.09\n\n4. **BERT-Tagger(Devlin et al., 2018)**:\n   - CTB5: Prec. = 95.86, Rec. = 96.26, F1 = 96.06\n   - CTB6: Prec. = 94.91, Rec. = 94.63, F1 = 94.77\n   - UD1.4: Prec. = 95.42, Rec. = 94.17, F1 = 94.79\n\n5. **BERT+FL**:\n   - CTB5: Prec. = 96.11, Rec. = 97.42, F1 = 96.76 (+0.70)\n   - CTB6: Prec. = 95.80, Rec. = 95.08, F1 = 95.44 (+0.67)\n   - UD1.4: Prec. = 96.33, Rec. = 95.85, F1 = 96.10 (+2.02)\n\n6. **BERT+DL**:\n   - CTB5: Prec. = 96.77, Rec. = 98.87, F1 = 97.81 (+1.75)\n   - CTB6: Prec. = 94.08, Rec. = 96.12, F1 = 95.09 (+0.32)\n   - UD1.4: Prec. = 96.10, Rec. = 97.79, F1 = 96.94 (+2.15)\n\n7. **BERT+DSC**:\n   - CTB5: Prec. = 97.10, Rec. = 98.75, F1 = 97.92 (+1.86)\n   - CTB6: Prec. = 96.29, Rec. = 96.85, F1 = 96.57 (+1.80)\n   - UD1.4: Prec. = 96.24, Rec. = 97.73, F1 = 96.98 (+2.19)\n\nThe table highlights the performance improvements achieved by models incorporating additional techniques such as Focal Loss (FL), Dynamic Loss (DL), and Discriminative Sample Classification (DSC). These improvements are indicated by the increments in F1 scores compared to the baseline BERT-Tagger model, shown in parentheses. The best-performing model across all datasets is **BERT+DSC**, which achieves the highest F1 scores in CTB5 (97.92), CTB6 (96.57), and UD1.4 (96.98). The table demonstrates the effectiveness of these techniques in enhancing POS tagging performance, particularly in handling easy examples and improving overall accuracy.",
    "segmentation": false
  },
  "image_4": {
    "image_id": 4,
    "image_path": "./example_output/images/image_4.jpg",
    "caption": [],
    "footnote": [
      "Table 4: Experimental results for English POS datasets. "
    ],
    "context": "We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development 4 Experiments In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after $p$ exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push $p$ to 1 as much as possible. The intuition of changing $p_{i1}$ to $(1-p_{i1})p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1})p_{i1}$ makes the model attach significantly less focus to them. A close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017) for object detection in vision. Focal loss was proposed for one-stage object detector to handle foreground-background tradeoff encountered during training. It down-weights the loss assigned to well-classified examples by adding a $(1-p)^{\\gamma}$ factor, leading the final loss to be $-(1-p)^{\\gamma}\\log p$ .  ",
    "chunk_order_index": 3,
    "chunk_id": "chunk-7796c957ecdc053bf1b6da735b8ebe5f",
    "description": "The image is a table summarizing experimental results for part-of-speech (POS) tagging on two English datasets: the Wall Street Journal (WSJ) and English Tweets. The table is divided into two sections, each corresponding to one dataset. The columns in the table are labeled 'Model,' 'Prec. (Precision),' 'Rec. (Recall),' and 'F1' (F1 Score). The rows represent different models and their performance metrics. Here is a detailed breakdown of the content:\n\n### **English WSJ Section**\n- **Models and Performance:**\n  - **Meta BiLSTM (Bohnet et al., 2018):** Precision and Recall are not provided, but the F1 score is 98.23.\n  - **BERT-Tagger (Devlin et al., 2018):** Precision is 99.21, Recall is 98.36, and F1 is 98.68.\n  - **BERT-Tagger+FL:** Precision is 98.36, Recall is 98.97, and F1 is 98.88 (+0.02 improvement over BERT-Tagger).\n  - **BERT-Tagger+DL:** Precision is 99.34, Recall is 98.22, and F1 is 98.91 (+0.05 improvement over BERT-Tagger).\n  - **BERT-Tagger+DSC:** Precision is 99.41, Recall is 98.93, and F1 is 99.38 (+0.52 improvement over BERT-Tagger).\n\n### **English Tweets Section**\n- **Models and Performance:**\n  - **FastText+CNN+CRF (Godin, 2019):** Precision and Recall are not provided, but the F1 score is 91.78.\n  - **BERT-Tagger (Devlin et al., 2018):** Precision is 92.33, Recall is 91.98, and F1 is 92.34.\n  - **BERT-Tagger+FL:** Precision is 91.24, Recall is 93.22, and F1 is 92.47 (+0.13 improvement over BERT-Tagger).\n  - **BERT-Tagger+DL:** Precision is 91.44, Recall is 92.88, and F1 is 92.52 (+0.18 improvement over BERT-Tagger).\n  - **BERT-Tagger+DSC:** Precision is 92.87, Recall is 93.54, and F1 is 92.58 (+0.24 improvement over BERT-Tagger).\n\n### **Key Observations:**\n- **English WSJ:** The BERT-Tagger+DSC model achieves the highest F1 score (99.38), which is a significant improvement (+0.52) over the baseline BERT-Tagger.\n- **English Tweets:** The BERT-Tagger+DSC model also achieves the highest F1 score (92.58), with a +0.24 improvement over the baseline BERT-Tagger.\n- **General Trends:** The addition of FL, DL, and DSC consistently improves the F1 scores across both datasets, with DSC showing the most substantial gains.\n\n### **Contextual Notes:**\n- The table is part of an evaluation of different models for POS tagging, including the baseline BERT-Tagger and variations that incorporate Focal Loss (FL), Derivative Loss (DL), and Dice Similarity Coefficient (DSC) losses.\n- The improvements are quantified as increments in F1 scores relative to the baseline BERT-Tagger model.\n- The context mentions that these losses are designed to address issues like class imbalance and focus on difficult examples during training, which aligns with the observed improvements in performance.\n\n### **Footnote and Caption:**\n- **Footnote:** Table 4: Experimental results for English POS datasets.\n- **Caption:** None provided in the image, but the table is clearly labeled for the two datasets: 'English WSJ' and 'English Tweets.'\n\n### **Overall Atmosphere:**\nThe table is structured and academic, presenting quantitative results in a clear and concise manner. It highlights the effectiveness of different loss functions in enhancing the performance of BERT-based models for POS tagging tasks on two distinct datasets.",
    "segmentation": false
  },
  "image_5": {
    "image_id": 5,
    "image_path": "./example_output/images/image_5.jpg",
    "caption": [],
    "footnote": [],
    "context": "ELMo: a tagging model with pretraining from Peters et al. (2018). Lattice-LSTM: Baselines We use the following baselines: Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 4.2 Named Entity Recognition   datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue. Table 4 presents the experimental results for English datasets. ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-16c9562e4fc9927ad1d945506d8bd2d2",
    "description": "The image is a table titled 'English CoNLL 2003' that presents the performance of various models on the CoNLL 2003 dataset, specifically focusing on Named Entity Recognition (NER). The table includes three columns: 'Model', 'Prec.' (Precision), 'Rec.' (Recall), and 'F1' (F1 Score). The models listed are as follows:\n\n1. **ELMo (Peters et al., 2018)**: Precision and Recall are not reported, but the F1 score is 92.22.\n2. **CVT (Clark et al., 2018)**: Precision and Recall are not reported, but the F1 score is 92.6.\n3. **BERT-Tagger (Devlin et al., 2018)**: Precision and Recall are not reported, but the F1 score is 92.8.\n4. **BERT-MRC (Li et al., 2019)**: Precision is 92.33, Recall is 94.61, and the F1 score is 93.04.\n5. **BERT-MRC+FL**: Precision is 93.13, Recall is 93.09, and the F1 score is 93.11, which is an improvement of +0.06 over BERT-MRC.\n6. **BERT-MRC+DL**: Precision is 93.22, Recall is 93.12, and the F1 score is 93.17, which is an improvement of +0.12 over BERT-MRC.\n7. **BERT-MRC+DSC**: Precision is 93.41, Recall is 93.25, and the F1 score is 93.33, which is an improvement of +0.29 over BERT-MRC.\n\nThe table highlights the performance of different models, with BERT-MRC serving as the baseline. Variants of BERT-MRC, such as those incorporating Focal Loss (FL), Dice Loss (DL), and DSC Loss, show incremental improvements in F1 scores. The DSC Loss variant achieves the highest F1 score of 93.33, outperforming the baseline by +0.29. The context provided indicates that the DSC Loss is particularly effective in addressing data imbalance issues, leading to significant performance gains compared to other loss functions like Focal Loss and Dice Loss.",
    "segmentation": false
  },
  "image_6": {
    "image_id": 6,
    "image_path": "./example_output/images/image_6.jpg",
    "caption": [],
    "footnote": [],
    "context": "ELMo: a tagging model with pretraining from Peters et al. (2018). Lattice-LSTM: Baselines We use the following baselines: Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 4.2 Named Entity Recognition  datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue. Table 4 presents the experimental results for English datasets.  ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-16c9562e4fc9927ad1d945506d8bd2d2",
    "description": "The image is a table titled 'English OntoNotes 5.0,' which presents the performance of various models on the task of Named Entity Recognition (NER). The table includes three columns: 'Model,' 'Prec.' (Precision), 'Rec.' (Recall), and 'F1' (F1 Score). The models and their corresponding performance metrics are as follows:\n\n1. **CVT (Clark et al., 2018)**:\n   - Precision: -\n   - Recall: -\n   - F1 Score: 88.8\n\n2. **BERT-Tagger (Devlin et al., 2018)**:\n   - Precision: 90.01\n   - Recall: 88.35\n   - F1 Score: 89.16\n\n3. **BERT-MRC (Li et al., 2019)**:\n   - Precision: 92.98\n   - Recall: 89.95\n   - F1 Score: 91.11\n\n4. **BERT-MRC+FL**:\n   - Precision: 90.13\n   - Recall: 92.34\n   - F1 Score: 91.22 (+0.11 compared to BERT-MRC)\n\n5. **BERT-MRC+DL**:\n   - Precision: 91.70\n   - Recall: 92.06\n   - F1 Score: 91.88 (+0.77 compared to BERT-MRC)\n\n6. **BERT-MRC+DSC**:\n   - Precision: 91.59\n   - Recall: 92.56\n   - F1 Score: 92.07 (+0.96 compared to BERT-MRC)\n\nThe table highlights the performance improvements achieved by incorporating different loss functions (FL: Focal Loss, DL: Dice Loss, DSC: Dice Similarity Coefficient Loss) into the BERT-MRC model. The BERT-MRC+DSC model achieves the highest F1 Score of 92.07, outperforming the baseline BERT-MRC model by 0.96. The precision and recall values also show consistent improvements across the modified models, indicating better performance in identifying and correctly classifying named entities in the text.",
    "segmentation": false
  },
  "image_7": {
    "image_id": 7,
    "image_path": "./example_output/images/image_7.jpg",
    "caption": [],
    "footnote": [
      "Table 5: Experimental results for NER task. "
    ],
    "context": "ELMo: a tagging model with pretraining from Peters et al. (2018). Lattice-LSTM: Baselines We use the following baselines: Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by Li et al. (2019) as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003) and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1. 4.2 Named Entity Recognition datasets. As can be seen, the proposed DSC loss outperforms the best baseline results by a large margin, i.e., outperforming BERT-tagger by $+1.86$ in terms of F1 score on CTB5, $+1.80$ on CTB6 and $+2.19$ on UD1.4. As far as we know, we are achieving SOTA performances on the three datasets. Focal loss only obtains a little performance improvement on CTB5 and CTB6, and the dice loss obtains huge gain on CTB5 but not on CTB6, which indicates the three losses are not consistently robust in solving the data imbalance issue. Table 4 presents the experimental results for English datasets.   ",
    "chunk_order_index": 4,
    "chunk_id": "chunk-16c9562e4fc9927ad1d945506d8bd2d2",
    "description": "The image is a table presenting experimental results for Named Entity Recognition (NER) tasks on two Chinese datasets: Chinese MSRA and Chinese OntoNotes 4.0. The table compares the performance of various models in terms of precision (Prec.), recall (Rec.), and F1 score. The models include baselines and variations of the BERT-MRC model with different loss functions. Here is a detailed breakdown of the table:\n\n### **Chinese MSRA**\n- **Models and Results:**\n  - **Lattice-LSTM (Zhang and Yang, 2018):**\n    - Precision: 93.57\n    - Recall: 92.79\n    - F1: 93.18\n  - **BERT-Tagger (Devlin et al., 2018):**\n    - Precision: 94.97\n    - Recall: 94.62\n    - F1: 94.80\n  - **Glyce-BERT (Wu et al., 2019):**\n    - Precision: 95.57\n    - Recall: 95.51\n    - F1: 95.54\n  - **BERT-MRC (Li et al., 2019):**\n    - Precision: 96.18\n    - Recall: 95.12\n    - F1: 95.75\n  - **BERT-MRC+FL (Focal Loss):**\n    - Precision: 95.45\n    - Recall: 95.89\n    - F1: 95.67\n    - Change in F1: -0.08\n  - **BERT-MRC+DL (Dice Loss):**\n    - Precision: 96.20\n    - Recall: 96.68\n    - F1: 96.44\n    - Change in F1: +0.69\n  - **BERT-MRC+DSC (DSC Loss):**\n    - Precision: 96.67\n    - Recall: 96.77\n    - F1: 96.72\n    - Change in F1: +0.97\n\n### **Chinese OntoNotes 4.0**\n- **Models and Results:**\n  - **Lattice-LSTM (Zhang and Yang, 2018):**\n    - Precision: 76.35\n    - Recall: 71.56\n    - F1: 73.88\n  - **BERT-Tagger (Devlin et al., 2018):**\n    - Precision: 78.01\n    - Recall: 80.35\n    - F1: 79.16\n  - **Glyce-BERT (Wu et al., 2019):**\n    - Precision: 81.87\n    - Recall: 81.40\n    - F1: 80.62\n  - **BERT-MRC (Li et al., 2019):**\n    - Precision: 82.98\n    - Recall: 81.25\n    - F1: 82.11\n  - **BERT-MRC+FL (Focal Loss):**\n    - Precision: 83.63\n    - Recall: 82.97\n    - F1: 83.30\n    - Change in F1: +1.19\n  - **BERT-MRC+DL (Dice Loss):**\n    - Precision: 83.97\n    - Recall: 84.05\n    - F1: 84.01\n    - Change in F1: +1.90\n  - **BERT-MRC+DSC (DSC Loss):**\n    - Precision: 84.22\n    - Recall: 84.72\n    - F1: 84.47\n    - Change in F1: +2.36\n\n### **Key Observations:**\n- The **BERT-MRC+DSC** model consistently achieves the highest F1 scores across both datasets, indicating superior performance compared to other models and loss functions.\n- The **DSC loss** provides significant improvements over the baseline BERT-MRC model, with increases of +0.97 on Chinese MSRA and +2.36 on Chinese OntoNotes 4.0.\n- The **Focal Loss (FL)** and **Dice Loss (DL)** also show improvements but are less consistent compared to DSC loss.\n- The **BERT-MRC** model without any additional loss functions serves as a strong baseline, outperforming other models like Lattice-LSTM and BERT-Tagger.\n\n### **Contextual Notes:**\n- The table is part of an experiment evaluating the effectiveness of different loss functions (DSC, Focal, Dice) on the BERT-MRC model for NER tasks.\n- The datasets used are Chinese MSRA and Chinese OntoNotes 4.0, which are commonly used benchmarks for evaluating NER models.\n- The performance metrics reported are span-level micro-averaged precision, recall, and F1 score.\n\n### **Footnote:**\n- The table is labeled as **Table 5: Experimental results for NER task**.\n- The context mentions that the proposed DSC loss outperforms the best baseline results by a large margin, achieving state-of-the-art (SOTA) performances on the datasets.\n- The focal loss and dice loss show inconsistent performance, indicating that DSC loss is more robust in addressing data imbalance issues.\n\n### **Overall Impression:**\nThe table effectively compares the performance of various models and loss functions, highlighting the superior performance of the BERT-MRC model with DSC loss across both datasets. The results are presented clearly with numerical values and percentage changes, making it easy to interpret the improvements achieved by the proposed approach.",
    "segmentation": false
  },
  "image_8": {
    "image_id": 8,
    "image_path": "./example_output/images/image_8.jpg",
    "caption": [],
    "footnote": [],
    "context": "Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning 4.4 Paraphrase Identification Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+1.25$ in terms of F1 score and $+0.84$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+1.46$ on EM and $+1.41$ on F1. enables learning bidirectional contexts.  We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019). Baselines We used the following baselines: QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction. XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that ",
    "chunk_order_index": 5,
    "chunk_id": "chunk-4fd82d2b7d357c2a0f7e60544808d341",
    "description": "The image is a table summarizing the experimental results of various models on three datasets: SQuAD v1.1, SQuAD v2.0, and QuoRef. The table compares the performance of different models using two metrics: Exact Match (EM) and F1 score. The models evaluated include QANet, BERT, XLNet, and their variants with additional techniques such as FL (Feature Learning), DL (Data Learning), and DSC (a proposed method). Here is a detailed breakdown of the table:\n\n### Columns:\n1. **Model**: Lists the models and their variants.\n2. **SQuAD v1.1**: Contains two sub-columns:\n   - **EM**: Exact Match scores for SQuAD v1.1.\n   - **F1**: F1 scores for SQuAD v1.1.\n3. **SQuAD v2.0**: Contains two sub-columns:\n   - **EM**: Exact Match scores for SQuAD v2.0.\n   - **F1**: F1 scores for SQuAD v2.0.\n4. **QuoRef**: Contains two sub-columns:\n   - **EM**: Exact Match scores for QuoRef.\n   - **F1**: F1 scores for QuoRef.\n\n### Rows:\n1. **QANet (Yu et al., 2018b)**:\n   - SQuAD v1.1: EM = 73.6, F1 = 82.7.\n   - SQuAD v2.0: EM = -, F1 = -.\n   - QuoRef: EM = 34.41, F1 = 38.26.\n\n2. **BERT (Devlin et al., 2018)**:\n   - SQuAD v1.1: EM = 84.1, F1 = 90.9.\n   - SQuAD v2.0: EM = 78.7, F1 = 81.9.\n   - QuoRef: EM = 58.44, F1 = 64.95.\n\n3. **BERT+FL**:\n   - SQuAD v1.1: EM = 84.67 (+0.57), F1 = 91.25 (+0.35).\n   - SQuAD v2.0: EM = 78.92 (+0.22), F1 = 82.20 (+0.30).\n   - QuoRef: EM = 60.78 (+2.34), F1 = 66.19 (+1.24).\n\n4. **BERT+DL**:\n   - SQuAD v1.1: EM = 84.83 (+0.73), F1 = 91.86 (+0.96).\n   - SQuAD v2.0: EM = 78.99 (+0.29), F1 = 82.88 (+0.98).\n   - QuoRef: EM = 62.03 (+3.59), F1 = 66.88 (+1.93).\n\n5. **BERT+DSC**:\n   - SQuAD v1.1: EM = 85.34 (+1.24), F1 = 91.97 (+1.07).\n   - SQuAD v2.0: EM = 79.02 (+0.32), F1 = 82.95 (+1.05).\n   - QuoRef: EM = 62.44 (+4.00), F1 = 67.52 (+2.57).\n\n6. **XLNet (Yang et al., 2019)**:\n   - SQuAD v1.1: EM = 88.95, F1 = 94.52.\n   - SQuAD v2.0: EM = 86.12, F1 = 88.79.\n   - QuoRef: EM = 64.52, F1 = 71.49.\n\n7. **XLNet+FL**:\n   - SQuAD v1.1: EM = 88.90 (-0.05), F1 = 94.55 (+0.03).\n   - SQuAD v2.0: EM = 87.04 (+0.92), F1 = 89.32 (+0.53).\n   - QuoRef: EM = 65.19 (+0.67), F1 = 72.34 (+0.85).\n\n8. **XLNet+DL**:\n   - SQuAD v1.1: EM = 89.13 (+0.18), F1 = 95.36 (+0.84).\n   - SQuAD v2.0: EM = 87.22 (+1.10), F1 = 89.44 (+0.65).\n   - QuoRef: EM = 65.77 (+1.25), F1 = 72.85 (+1.36).\n\n9. **XLNet+DSC**:\n   - SQuAD v1.1: EM = 89.79 (+0.84), F1 = 95.77 (+1.25).\n   - SQuAD v2.0: EM = 87.65 (+1.53), F1 = 89.51 (+0.72).\n   - QuoRef: EM = 65.98 (+1.46), F1 = 72.90 (+1.41).\n\n### Observations:\n- **BERT+DSC** and **XLNet+DSC** show significant improvements over their base models (BERT and XLNet) across all datasets and metrics.\n- **XLNet+DSC** achieves the highest scores overall, particularly on SQuAD v1.1 and SQuAD v2.0.\n- The proposed DSC method consistently boosts performance, especially on SQuAD v1.1 and QuoRef, with notable gains in both EM and F1 scores.\n- The improvements are more pronounced on SQuAD v1.1 and QuoRef compared to SQuAD v2.0.\n\n### Contextual Notes:\n- The DSC loss is highlighted as a proposed method that significantly enhances performance.\n- The table demonstrates the effectiveness of the DSC method in improving both EM and F1 scores across the datasets.\n- The comparison includes baselines like QANet, BERT, and XLNet, showcasing the advancements made by incorporating additional techniques like FL, DL, and DSC.\n",
    "segmentation": false
  },
  "image_9": {
    "image_id": 9,
    "image_path": "./example_output/images/image_9.jpg",
    "caption": [
      "Table 6: Experimental results for MRC task. "
    ],
    "footnote": [
      "Table 7: Experimental results for PI task. "
    ],
    "context": "Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning 4.4 Paraphrase Identification Results Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by $+1.25$ in terms of F1 score and $+0.84$ in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by $+1.46$ on EM and $+1.41$ on F1. enables learning bidirectional contexts. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016, 2018) and Quoref (Dasigi et al., 2019). Baselines We used the following baselines: QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction. XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that  ",
    "chunk_order_index": 5,
    "chunk_id": "chunk-4fd82d2b7d357c2a0f7e60544808d341",
    "description": "The image is a table titled 'Table 6: Experimental results for MRC task.' It presents the performance of various models on two tasks: MRPC (Microsoft Research Paraphrase Corpus) and QQP (Quora Question Pairs), measured using the F1 score. The table compares the performance of BERT and XLNet models, both in their base forms and when enhanced with different loss functions: FL (Focal Loss), DL (Distillation Loss), and DSC (the proposed method). Here is a detailed breakdown of the table's content:\n\n### Table Structure:\n- **Columns**:\n  - **Model**: Lists the models being compared.\n  - **MRPC F1**: Reports the F1 score for the MRPC task.\n  - **QQP F1**: Reports the F1 score for the QQP task.\n\n- **Rows**:\n  - **BERT (Devlin et al., 2018)**: The baseline BERT model.\n  - **BERT+FL**: BERT enhanced with Focal Loss.\n  - **BERT+DL**: BERT enhanced with Distillation Loss.\n  - **BERT+DSC**: BERT enhanced with the proposed DSC loss.\n  - **XLNet (Yang et al., 2019)**: The baseline XLNet model.\n  - **XLNet+FL**: XLNet enhanced with Focal Loss.\n  - **XLNet+DL**: XLNet enhanced with Distillation Loss.\n  - **XLNet+DSC**: XLNet enhanced with the proposed DSC loss.\n\n### Detailed Values:\n#### BERT Models:\n1. **BERT (Devlin et al., 2018)**:\n   - MRPC F1: 88.0\n   - QQP F1: 91.3\n\n2. **BERT+FL**:\n   - MRPC F1: 88.43 (+0.43)\n   - QQP F1: 91.86 (+0.56)\n\n3. **BERT+DL**:\n   - MRPC F1: 88.71 (+0.71)\n   - QQP F1: 91.92 (+0.62)\n\n4. **BERT+DSC**:\n   - MRPC F1: 88.92 (+0.92)\n   - QQP F1: 92.11 (+0.81)\n\n#### XLNet Models:\n5. **XLNet (Yang et al., 2019)**:\n   - MRPC F1: 89.2\n   - QQP F1: 91.8\n\n6. **XLNet+FL**:\n   - MRPC F1: 89.25 (+0.05)\n   - QQP F1: 92.31 (+0.51)\n\n7. **XLNet+DL**:\n   - MRPC F1: 89.33 (+0.13)\n   - QQP F1: 92.39 (+0.59)\n\n8. **XLNet+DSC**:\n   - MRPC F1: 89.78 (+0.58)\n   - QQP F1: 92.60 (+0.79)\n\n### Observations:\n- **Performance Boost with DSC**: The proposed DSC loss consistently improves the F1 scores for both MRPC and QQP tasks across both BERT and XLNet models. For example:\n  - BERT+DSC achieves an F1 score of 88.92 on MRPC (+0.92) and 92.11 on QQP (+0.81).\n  - XLNet+DSC achieves an F1 score of 89.78 on MRPC (+0.58) and 92.60 on QQP (+0.79).\n  \n- **Comparison Between BERT and XLNet**: XLNet generally outperforms BERT in both tasks, even in their baseline forms. For instance:\n  - XLNet (baseline) achieves 89.2 on MRPC and 91.8 on QQP, compared to BERT's 88.0 and 91.3, respectively.\n  \n- **Impact of Other Losses**: While Focal Loss (FL) and Distillation Loss (DL) also improve performance, the gains are smaller compared to DSC. For example:\n  - BERT+FL improves MRPC by +0.43 and QQP by +0.56.\n  - XLNet+DL improves MRPC by +0.13 and QQP by +0.59.\n\n### Contextual Notes:\n- The table is part of an experimental evaluation for the MRC (Machine Reading Comprehension) task.\n- The proposed DSC loss is highlighted as a significant contributor to performance improvements.\n- The datasets used are MRPC and QQP, which are standard benchmarks for paraphrase identification tasks.\n\n### Overall:\nThe table effectively demonstrates the effectiveness of the proposed DSC loss in enhancing the performance of both BERT and XLNet models on paraphrase identification tasks, with notable improvements in F1 scores across both datasets.",
    "segmentation": false
  },
  "image_10": {
    "image_id": 10,
    "image_path": "./example_output/images/image_10.jpg",
    "caption": [],
    "footnote": [],
    "context": "Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that  We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50\\%$ being positive and $50\\%$ being negative. Positive and negative augmentation ( $\\mathbf{\\Psi}+$ positive $\\pmb{\\&}$ +negative) We augmented the original training data with additional positive and negative examples with the data distribution staying the same. The augmented dataset contains 458,477 examples, with $50\\%$ being positive and $50\\%$ being negative. Negative downsampling (- negative)  training examples in the original training set as templates. Then we used Spacy to retrieve entity mentions and replace them with new ones by linking mentions to their corresponding entities in DBpedia. The augmented set contains 458,477 examples, with $50\\%$ being positive and $50\\%$ being negative. Negative augmentation ( $\\mp$ negative) We created a more imbalanced dataset. The size of the newly constructed training set and the data augmented technique are exactly the same as +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21\\%$ being positive and $79\\%$ being negative. ",
    "chunk_order_index": 5,
    "chunk_id": "chunk-4fd82d2b7d357c2a0f7e60544808d341",
    "description": "The image is a table presenting performance metrics for different models and data augmentation strategies. The table is structured with rows representing different models (BERT, BERT+FL, BERT+DL, BERT+DSC) and columns representing various data augmentation scenarios. The columns are labeled as follows: 'original', '+ positive', '+ negative', '- negative', and '+ positive & negative'. Each cell contains a numerical value representing the performance metric (likely accuracy or a similar score) for the corresponding model and augmentation scenario. Here is a detailed breakdown of the table:\n\n### Rows:\n1. **BERT**:\n   - Original: 91.3\n   - + positive: 92.27\n   - + negative: 90.08\n   - - negative: 89.73\n   - + positive & negative: 93.14\n\n2. **BERT+FL**:\n   - Original: 91.86 (+0.56)\n   - + positive: 92.64 (+0.37)\n   - + negative: 90.61 (+0.53)\n   - - negative: 90.79 (+1.06)\n   - + positive & negative: 93.45 (+0.31)\n\n3. **BERT+DL**:\n   - Original: 91.92 (+0.62)\n   - + positive: 92.87 (+0.60)\n   - + negative: 90.22 (+0.14)\n   - - negative: 90.49 (+0.76)\n   - + positive & negative: 93.52 (+0.38)\n\n4. **BERT+DSC**:\n   - Original: 92.11 (+0.81)\n   - + positive: 92.92 (+0.65)\n   - + negative: 90.78 (+0.70)\n   - - negative: 90.80 (+1.07)\n   - + positive & negative: 93.63 (+0.49)\n\n### Observations:\n- The 'original' column shows the baseline performance of each model without any data augmentation.\n- The '+ positive' column indicates the performance when additional positive examples are included.\n- The '+ negative' column shows the performance when additional negative examples are included.\n- The '- negative' column represents the performance when negative examples are down-sampled.\n- The '+ positive & negative' column shows the performance when both positive and negative examples are augmented.\n\n### Key Trends:\n- All models show improvements in performance when data augmentation is applied, particularly in the '+ positive & negative' scenario.\n- BERT+DSC consistently shows the highest performance across all augmentation scenarios.\n- The '+ positive & negative' augmentation generally yields the best results for all models, indicating that balancing the dataset with both positive and negative examples is beneficial.\n- The '- negative' augmentation (down-sampling negative examples) generally results in lower performance compared to other augmentation strategies.\n\n### Contextual Notes:\n- The table is part of a study evaluating the impact of different data augmentation techniques on model performance.\n- The augmentation strategies include balancing the dataset, adding positive examples, adding negative examples, down-sampling negative examples, and combining positive and negative augmentations.\n- The performance metric (likely accuracy) is presented for each scenario, with improvements over the baseline (original) highlighted in parentheses (e.g., +0.56 for BERT+FL in the 'original' column).",
    "segmentation": false
  },
  "image_11": {
    "image_id": 11,
    "image_path": "./example_output/images/image_11.jpg",
    "caption": [
      "Table 8: The effect of different data augmentation ways for QQP in terms of F1-score. "
    ],
    "footnote": [],
    "context": "DSC achieves the highest F1 score across all datasets. Specially, for $^+$ positive, DSC achieves minor improvements $\\left(+0.05\\,\\mathrm{F}1\\right)$ ) over DL. In contrast, it significantly outperforms DL Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that $^+$ positive outperforms original, and +negative underperforms original. This is in line with our expectation since $^+$ positive creates a balanced dataset while +negative creates a more imbalanced dataset. Despite the fact that -negative creates a balanced dataset, the number of training data decreases, resulting in inferior performances. +negative, except that we chose negative training examples as templates. The augmented training set contains 458,477 examples, with $21\\%$ being positive and $79\\%$ being negative.   Negative downsampling (- negative) We down-sampled negative examples in the original training set to get a balanced training set. The down-sampled set contains 269,165 examples, with $50\\%$ being positive and $50\\%$ being negative. Positive and negative augmentation ( $\\mathbf{\\Psi}+$ positive $\\pmb{\\&}$ +negative) We augmented the original training data with additional positive and negative examples with the data distribution staying the same. The augmented dataset contains 458,477 examples, with $50\\%$ being positive and $50\\%$ being negative. ",
    "chunk_order_index": 5,
    "chunk_id": "chunk-4fd82d2b7d357c2a0f7e60544808d341",
    "description": "The image is a table labeled 'Table 8: The effect of different data augmentation ways for QQP in terms of F1-score.' The table compares the performance of various models on two datasets: SST-2 and SST-5, measured by accuracy (Acc). The table has three rows, each representing a different model configuration, and two columns for the datasets. Here is a detailed breakdown:\n\n  - **Columns**:\n    - The first column is labeled 'Model,' indicating the different model configurations being compared.\n    - The second column is labeled 'SST-2 Acc,' showing the accuracy scores for the SST-2 dataset.\n    - The third column is labeled 'SST-5 Acc,' showing the accuracy scores for the SST-5 dataset.\n\n  - **Rows**:\n    1. **BERT+CE**:\n       - SST-2 Acc: 94.90\n       - SST-5 Acc: 55.57\n    2. **BERT+DL**:\n       - SST-2 Acc: 94.37\n       - SST-5 Acc: 54.63\n    3. **BERT+DSC**:\n       - SST-2 Acc: 94.84\n       - SST-5 Acc: 55.19\n\n  - **Observations**:\n    - For the SST-2 dataset, BERT+CE achieves the highest accuracy (94.90), followed closely by BERT+DSC (94.84), and BERT+DL has the lowest accuracy (94.37).\n    - For the SST-5 dataset, BERT+CE also achieves the highest accuracy (55.57), followed by BERT+DSC (55.19), and BERT+DL has the lowest accuracy (54.63).\n    - Across both datasets, BERT+CE consistently performs the best, while BERT+DL performs the worst. BERT+DSC shows intermediate performance.\n\n  - **General Pattern**:\n    - The performance differences between the models are relatively small, especially for SST-2, where all models achieve high accuracy.\n    - For SST-5, the differences are slightly more pronounced, but the overall performance is lower compared to SST-2.\n\n  The table is presented in a clean, tabular format with numerical values clearly aligned under their respective columns. There are no additional visual elements or colors, and the focus is purely on the comparative performance of the models.",
    "segmentation": false
  },
  "image_12": {
    "image_id": 12,
    "image_path": "./example_output/images/image_12.jpg",
    "caption": [],
    "footnote": [],
    "context": "Bernd Bohnet, Ryan T. McDonald, Gon¸calo Simo˜es, Daniel Andor, Emily Pitler, and Joshua Maynez. 2018. Morphosyntactic tagging with a meta-bilstm model over context sensitive token encodings. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2642–2652. References We thank all anonymous reviewers, as well as Qinghong Han, Wei Wu and Jiawei Wu for their comments and suggestions. The work is supported by the National Natural Science Foundation of China (NSFC No. 61625107 and 61751209). Acknowledgement to achieve significant performance boost without changing model architectures.  $\\alpha$ is set to 0.6 while for QuoRef, the highest F1 is 68.44 when $\\alpha$ is set to 0.4. In addition, we can observe that the performance varies a lot as $\\alpha$ changes in distinct datasets, which shows that the hyperparameters $\\alpha,\\beta$ acturally play an important role in TI. 6 Conclusion In this paper, we propose the dice-based loss to narrow down the gap between training objective and evaluation metrics (F1 score). Experimental results show that the proposed loss function help Table 10: The effect of hyperparameters in Tversky Index. We set $\\beta=1-\\alpha$ and thus we only list $\\alpha$ here. ",
    "chunk_order_index": 6,
    "chunk_id": "chunk-fbdf9d64766ea51ff2a2526d92ed5d67",
    "description": "The image is a table titled 'The effect of hyperparameters in Tversky Index.' It shows the impact of the hyperparameter \\\\( \\\\alpha \\\\) on performance metrics for two datasets: 'Chinese Onto4.0' and 'English QuoRef.' The table is structured with three columns: the first column lists the values of \\\\( \\\\alpha \\\\) ranging from 0.1 to 0.9, the second column presents the corresponding performance scores for the 'Chinese Onto4.0' dataset, and the third column shows the performance scores for the 'English QuoRef' dataset. The performance scores are represented as numerical values, likely indicating F1 scores or similar metrics. Key observations include: \\n- For the 'Chinese Onto4.0' dataset, the highest performance score is 84.67, achieved when \\\\( \\\\alpha = 0.6 \\\\). \\n- For the 'English QuoRef' dataset, the highest performance score is 68.44, achieved when \\\\( \\\\alpha = 0.4 \\\\). \\n- The performance scores for both datasets fluctuate as \\\\( \\\\alpha \\\\) changes, indicating that the hyperparameter \\\\( \\\\alpha \\\\) plays a significant role in tuning the model's performance. \\n- The table highlights the variability in performance across different values of \\\\( \\\\alpha \\\\), suggesting that optimal hyperparameter settings may differ between datasets. \\n- The context provided in the caption indicates that \\\\( \\\\beta \\\\) is set as \\\\( 1 - \\\\alpha \\\\), and the focus is on the effect of \\\\( \\\\alpha \\\\) alone. The table is part of an experimental analysis to evaluate the impact of hyperparameters on the Tversky Index in a computational linguistics or machine learning context.",
    "segmentation": false
  }
}