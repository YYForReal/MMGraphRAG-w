<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d2" for="node" attr.name="source_id" attr.type="string"/>
<key id="d1" for="node" attr.name="description" attr.type="string"/>
<key id="d0" for="node" attr.name="entity_type" attr.type="string"/>
<graph edgedefault="undirected"><node id="&quot;IMAGE_3&quot;">
  <data key="d0">"ORI_IMG"</data>
  <data key="d1">**IMAGE_3** is a table titled *"Experimental results for Chinese POS datasets including CTB5, CTB6, and UD1.4."* It compares the performance of multiple models on part-of-speech (POS) tagging tasks across three datasets: CTB5, CTB6, and UD1.4. The table includes metrics such as Precision (Prec.), Recall (Rec.), and F1 score for each model.  

The evaluated models and their results are as follows:  
1. **Joint-POS(Sig)(Shao et al., 2017)**:  
   - CTB5: Prec. = 93.68, Rec. = 94.47, F1 = 94.07  
   - CTB6: F1 = 90.81 (Prec. and Rec. not reported)  
   - UD1.4: Prec. = 89.28, Rec. = 89.54, F1 = 89.41  

2. **Joint-POS(Ens)(Shao et al., 2017)**:  
   - CTB5: Prec. = 93.95, Rec. = 94.81, F1 = 94.38  
   - CTB6: No metrics reported  
   - UD1.4: Prec. = 89.67, Rec. = 89.86, F1 = 89.75  

3. **Lattice-LSTM(Zhang and Yang, 2018)**:  
   - CTB5: Prec. = 94.77, Rec. = 95.51, F1 = 95.14  
   - CTB6: Prec. = 92.00, Rec. = 90.86, F1 = 91.43  
   - UD1.4: Prec. = 90.47, Rec. = 89.70, F1 = 90.09  

4. **BERT-Tagger(Devlin et al., 2018)**:  
   - CTB5: Prec. = 95.86, Rec. = 96.26, F1 = 96.06  
   - CTB6: Prec. = 94.91, Rec. = 94.63, F1 = 94.77  
   - UD1.4: Prec</data>
  <data key="d2">./example_output/images/image_3.jpg</data>
</node>
</graph></graphml>